# Seelie Client å®‰è£…ä¸ä½¿ç”¨æ•™ç¨‹

> [Seelieæœºå™¨å­¦ä¹ å¹³å°](https://ml.ssr.mihoyo.com)

# å®‰è£…å’Œé…ç½®token

1. å®¢æˆ·ç«¯ä¸‹è½½: å¹³å°ä¸»é¡µå³ä¸Šè§’
2. tokenè·å–: å¹³å°ä¸»é¡µå³ä¸Šè§’
3. å®‰è£…å¹¶**é…ç½®å®¢æˆ·ç«¯**

   ```bash
   # è§£å‹
   tar -zxvf seelie-client-xxx.tar.gz
   # å®‰è£…
   sudo mv seelie /usr/local/bin
   
   ##  macosé¦–æ¬¡æ‰§è¡Œseelieå‘½ä»¤ï¼Œå¯èƒ½éœ€è¦åœ¨ `ç³»ç»Ÿåå¥½è®¾ç½®` ã€‹`å®‰å…¨æ€§å’Œéšç§` é‡Œæˆæƒå…è®¸è¿è¡Œ
   # ç”Ÿæˆé…ç½®æ–‡ä»¶
   seelie config --help
   
   # tokenå¯ä»¥åœ¨å¹³å°ä¸»é¡µå³ä¸Šè§’è·å–
   seelie config init -H ml.ssr.mihoyo.com -P 443 -p https -t æˆ‘çš„token
   
   # é…ç½®è‡ªåŠ¨è¡¥å…¨ï¼Œéå¿…é¡» 
   ## å¯¹äºä½¿ç”¨bash shellçš„ç”¨æˆ·
   seelie completion bash --help
   ## å¯¹äºä½¿ç”¨zsh shellçš„ç”¨æˆ·
   seelie completion zsh --help
   # æ ¹æ®æç¤ºæ“ä½œåï¼Œé‡æ–°æ‰“å¼€ç»ˆç«¯çª—å£å³å¯
   ```

# ä½¿ç”¨

1. æŸ¥çœ‹å¸®åŠ©æ–‡æ¡£
   ```bash
   # æ¯ä¸€çº§å­å‘½ä»¤éƒ½å¸¦å¸®åŠ©æ–‡æ¡£å’ŒexampleğŸŒ°
   seelie -h
   seelie submit -h 
   seelie submit tfjob -h
   ```

2. æ³¨å†ŒNASæµè§ˆå™¨ç”¨æˆ·

   > seelieåç«¯æ‰€åœ¨çš„ç½‘ç»œç¯å¢ƒæ— æ³•è®¿é—®è®¡ç®—é›†ç¾¤æ‰€åœ¨çš„naså­˜å‚¨ï¼Œå› æ­¤éœ€è¦ç”¨æˆ·æ‰‹åŠ¨æ‰§è¡Œæ³¨å†Œå‘½ä»¤

   ```shell
   seelie data register-user
   # å¦‚ä¸‹è¾“å‡ºï¼Œæç¤ºæ³¨å†ŒæˆåŠŸæˆ–è€…å·²æ³¨å†Œ
   Using config file: /Users/shuai.yang/.seelie/config.yaml
   2022/12/06 11:56:25 [INFO] cluster: aws-dev, nas storage for data: https://ml-dev-aws.eks.hoyoverse.com/nfsdata
   2022/12/06 11:56:25 [WARN] user<shuai.yang> already exist for nas storage(data)
   2022/12/06 11:56:25 [INFO] cluster: aws-dev, nas storage for jupyter: https://ml-dev-aws.eks.hoyoverse.com/nfsjupyter
   2022/12/06 11:56:26 [WARN] user<shuai.yang> already exist for nas storage(jupyter)
   2022/12/06 11:56:26 [INFO] cluster: dev, nas storage for data: https://ml-dev.ssr.mihoyo.com/nfsdata
   2022/12/06 11:56:26 [WARN] user<shuai.yang> already exist for nas storage(data)
   2022/12/06 11:56:26 [INFO] cluster: dev, nas storage for jupyter: https://ml-dev.ssr.mihoyo.com/nfsjupyter
   2022/12/06 11:56:26 [WARN] user<shuai.yang> already exist for nas storage(jupyter)
   2022/12/06 11:56:26 [INFO] cluster: aliyun-sh, nas storage for data: https://ml-aliyun-sh.ssr.mihoyo.com/nfsdata
   2022/12/06 11:56:26 [WARN] user<shuai.yang> already exist for nas storage(data)
   2022/12/06 11:56:26 [INFO] cluster: aliyun-sh, nas storage for jupyter: https://ml-aliyun-sh.ssr.mihoyo.com/nfsjupyter
   2022/12/06 11:56:26 [WARN] user<shuai.yang> already exist for nas storage(jupyter)
   ```
   
   + `mihoyo.com` åŸŸåä¸‹çš„nasæµè§ˆå™¨ï¼Œè‡ªåŠ¨ç™»å½•ã€‚åªè¦æµè§ˆå™¨ç™»å½•è¿‡kmæˆ–è€…`https://op.mihoyo.com/#/home`
   + `hoyoverse.com`åŸŸåæ²¡æœ‰ssoï¼Œéœ€è¦ä½¿ç”¨ä»£ç†ç™»å½•: `https://ml-dev-aws.eks.hoyoverse.com/seelie/login-assist?token=ä½ çš„token`

3. åˆ›å»ºå•æœºjob
   > å•æœºjobä¸ç®—æ³•æ¡†æ¶æ— å…³,é‡Œé¢å¯ä»¥è¿è¡Œä»»æ„æ¡†æ¶çš„jobï¼›è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨tfjobå³å¯(tensorflow framework)

   ```bash
   seelie submit tfjob --name "test job" --description "submit by seelie cli" \
   --cluster dev --namespace default --cpu 2 --memory 8 --gpu 0 --worker-count 1 \
   --image registry.cn-shanghai.aliyuncs.com/shuaiyy/mihoyo-ai:tf2.4.3-gpu-jupyter-lab \
   --entrypoint-type "bash -c" --entrypoint "sleep 10m; echo failed; exit 1" \
   -E enable_ema=1
   ```

4. åˆ›å»ºTFåˆ†å¸ƒå¼è®­ç»ƒ
   > 2 ps + 4 worker;
   > 
   > data_dir=/data_dir/mnist_data  train_steps=30  batch_size=32

   ```shell
   seelie submit tfjob --name "dist-tf-2ps-4worker" --description "submit by seelie cli" \
    --cluster dev --namespace default --cpu 6 --memory 12 --gpu 0 --worker-count 4 \
    --image registry.cn-shanghai.aliyuncs.com/shuaiyy/2233:tf1.5-dist-mnist-demo-train_op1.4 \
    --entrypoint-type "python" --entrypoint "/workspace/dist_mnist.py" \
    -E enable_ema=1 -E env_aaa=test \
    -T data_dir=/data_dir/mnist_data -T train_steps=30 -T batch_size=32 \
    -M m_over_sale=3 -M m_enable_debug_toolbox=true -M m_instance_retain=true -M m_retain_time=1h \
    --ps-count 2 --ps-cpu 4 --ps-memory 20
   ```

5. æŸ¥çœ‹job
   `seelie job get --job_id=123`
6. stop job
   `seelie job stop --job_id=123`